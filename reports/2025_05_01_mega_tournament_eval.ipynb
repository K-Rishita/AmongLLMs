{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deception ELO!\n",
    "\n",
    "An all-models, 810-game war on Among Us to see who is the best at deceptive capability. There will be blood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, json_normalize\n",
    "from pprint import pprint as pp\n",
    "from typing import List, Dict, Any, Tuple, Union, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_analysis_elo_winrate(\n",
    "    games: List[Dict],\n",
    "    models: List[str],\n",
    "    n_bootstrap: int = 1000,\n",
    "    confidence_level: float = 0.95,\n",
    "    K: int = 32,\n",
    "    BASE_ELO: int = 1500,\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Perform bootstrap analysis on game data to get confidence intervals for ELO and win rates.\n",
    "\n",
    "    Args:\n",
    "        games: List of game dictionaries containing game results\n",
    "        models: List of model names to analyze\n",
    "        n_bootstrap: Number of bootstrap samples to generate\n",
    "        confidence_level: Confidence level for intervals (e.g., 0.95 for 95% CI)\n",
    "        K: ELO K-factor\n",
    "        BASE_ELO: Base ELO rating\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (elo_results, win_rate_results) dictionaries containing mean and CI for each model\n",
    "    \"\"\"\n",
    "    # Initialize results dictionaries\n",
    "    elo_results = {model: {\"samples\": []} for model in models}\n",
    "    win_rate_results = {model: {\"samples\": []} for model in models}\n",
    "\n",
    "    # Function to update ELO ratings\n",
    "    def update_elo(winner_elo, loser_elo):\n",
    "        expected_win = 1 / (1 + 10 ** ((loser_elo - winner_elo) / 400))\n",
    "        change = K * (1 - expected_win)\n",
    "        return winner_elo + change, loser_elo - change\n",
    "\n",
    "    # Perform bootstrap resampling\n",
    "    for bootstrap_iter in range(n_bootstrap):\n",
    "        if bootstrap_iter % 100 == 0:\n",
    "            print(f\"Bootstrap iteration {bootstrap_iter}/{n_bootstrap}\")\n",
    "\n",
    "        # Sample games with replacement\n",
    "        bootstrap_games = random.choices(games, k=len(games))\n",
    "\n",
    "        # Process the bootstrap sample\n",
    "        impostor_elo = defaultdict(lambda: BASE_ELO)\n",
    "        win_counts = defaultdict(lambda: {\"wins\": 0, \"games\": 0})\n",
    "\n",
    "        # Process each game in this bootstrap sample\n",
    "        for game in bootstrap_games:\n",
    "            impostor_models = []\n",
    "            crewmate_models = []\n",
    "            all_models = []\n",
    "            impostor_won = game[\"winner\"] == 1 or game[\"winner\"] == 4\n",
    "\n",
    "            for player in game:\n",
    "                if player.startswith(\"Player\"):\n",
    "                    model = game[player][\"model\"]\n",
    "                    all_models.append(model)\n",
    "                    if game[player][\"identity\"] == \"Impostor\":\n",
    "                        impostor_models.append(model)\n",
    "                    else:\n",
    "                        crewmate_models.append(model)\n",
    "\n",
    "            # Update ELO only for impostors (Deception ELO)\n",
    "            if impostor_models and crewmate_models:\n",
    "                avg_crewmate_elo = sum(impostor_elo[m] for m in crewmate_models) / len(\n",
    "                    crewmate_models\n",
    "                )\n",
    "                for impostor in impostor_models:\n",
    "                    if impostor_won:\n",
    "                        impostor_elo[impostor], _ = update_elo(\n",
    "                            impostor_elo[impostor], avg_crewmate_elo\n",
    "                        )\n",
    "                    else:\n",
    "                        _, impostor_elo[impostor] = update_elo(\n",
    "                            avg_crewmate_elo, impostor_elo[impostor]\n",
    "                        )\n",
    "\n",
    "            # Update win counts for all players\n",
    "            for model in all_models:\n",
    "                win_counts[model][\"games\"] += 1\n",
    "                if (model in impostor_models and impostor_won) or (\n",
    "                    model not in impostor_models and not impostor_won\n",
    "                ):\n",
    "                    win_counts[model][\"wins\"] += 1\n",
    "\n",
    "        # Calculate win rates for this bootstrap sample\n",
    "        win_rates = {\n",
    "            model: win_counts[model][\"wins\"] / win_counts[model][\"games\"]\n",
    "            if win_counts[model][\"games\"] > 0\n",
    "            else 0\n",
    "            for model in models\n",
    "        }\n",
    "\n",
    "        # Store results for this bootstrap iteration\n",
    "        for model in models:\n",
    "            elo_results[model][\"samples\"].append(impostor_elo[model])\n",
    "            win_rate_results[model][\"samples\"].append(win_rates.get(model, 0))\n",
    "\n",
    "    # Calculate statistics from bootstrap samples\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = alpha / 2 * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "\n",
    "    for model in models:\n",
    "        # ELO statistics\n",
    "        elo_samples = elo_results[model][\"samples\"]\n",
    "        elo_results[model][\"mean\"] = np.mean(elo_samples)\n",
    "        elo_results[model][\"ci_lower\"] = np.percentile(elo_samples, lower_percentile)\n",
    "        elo_results[model][\"ci_upper\"] = np.percentile(elo_samples, upper_percentile)\n",
    "\n",
    "        # Win rate statistics\n",
    "        win_rate_samples = win_rate_results[model][\"samples\"]\n",
    "        win_rate_results[model][\"mean\"] = np.mean(win_rate_samples)\n",
    "        win_rate_results[model][\"ci_lower\"] = np.percentile(\n",
    "            win_rate_samples, lower_percentile\n",
    "        )\n",
    "        win_rate_results[model][\"ci_upper\"] = np.percentile(\n",
    "            win_rate_samples, upper_percentile\n",
    "        )\n",
    "\n",
    "    return elo_results, win_rate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_PATH: str = \"../expt-logs/\"\n",
    "# EXPT_NAME: str = \"2025-02-24_deception_elo_v3\" (original, random tournament)\n",
    "EXPT_NAME: str = \"2025-04-21_deception_elo_1on1\"\n",
    "summary_df_path: str = os.path.join(LOGS_PATH, EXPT_NAME, \"summary.json\")\n",
    "\n",
    "with open(summary_df_path, \"r\") as f:\n",
    "    games = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = [list(game.values())[0] for game in games]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games[0][\"Player 1\"][\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = set()\n",
    "for game in games:\n",
    "    for player_key in game:\n",
    "        if player_key.startswith(\"Player \"):\n",
    "            model = game[player_key][\"model\"]\n",
    "            all_models.add(model)\n",
    "\n",
    "models: List[str] = sorted(list(all_models))\n",
    "\n",
    "pp(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expt 1: Deception ELO v Win Rate\n",
    "\n",
    "As a measure of how much models win with/without being deceptive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_results, win_rate_results = bootstrap_analysis_elo_winrate(\n",
    "    games, models, n_bootstrap=1000, confidence_level=0.90\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_elo_vs_winrate_with_ci(\n",
    "    elo_results: Dict, win_rate_results: Dict, models: List[str]\n",
    ") -> go.Figure:\n",
    "    colors = [\n",
    "        \"#1f77b4\",\n",
    "        \"#d62728\",\n",
    "        \"#2ca02c\",\n",
    "        \"#9467bd\",\n",
    "        \"#8c564b\",\n",
    "        \"#e377c2\",\n",
    "        \"#7f7f7f\",\n",
    "        \"#bcbd22\",\n",
    "        \"#17becf\",\n",
    "        \"#ff7f0e\",\n",
    "        \"#7f7f7f\",\n",
    "        \"#bcbd22\",\n",
    "        \"#17becf\",\n",
    "        \"#ff7f0e\",\n",
    "    ]\n",
    "    model_labels = [model.split(\"/\")[-1] for model in models]\n",
    "    textpositions = [\n",
    "        \"top left\",\n",
    "        \"top center\",\n",
    "        \"middle right\",\n",
    "        \"middle right\",\n",
    "        \"middle right\",\n",
    "        \"bottom right\",\n",
    "        \"bottom right\",\n",
    "        \"bottom center\",\n",
    "        \"middle left\",\n",
    "        \"bottom right\",\n",
    "        \"top center\",\n",
    "    ]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        x_center = win_rate_results[model][\"mean\"] * 100\n",
    "        y_center = elo_results[model][\"mean\"]\n",
    "        x_radius = (\n",
    "            win_rate_results[model][\"ci_upper\"] - win_rate_results[model][\"ci_lower\"]\n",
    "        ) * 50\n",
    "        y_radius = (elo_results[model][\"ci_upper\"] - elo_results[model][\"ci_lower\"]) / 2\n",
    "        t = np.linspace(0, 2 * np.pi, 100)\n",
    "\n",
    "        # Inner ellipse (higher confidence)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_center + 0.5 * x_radius * np.cos(t),\n",
    "                y=y_center + 0.5 * y_radius * np.sin(t),\n",
    "                fill=\"toself\",\n",
    "                fillcolor=colors[i],\n",
    "                opacity=0.15,\n",
    "                line=dict(color=colors[i]),\n",
    "                showlegend=False,\n",
    "                hoverinfo=\"skip\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Outer ellipse (lower confidence)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_center + x_radius * np.cos(t),\n",
    "                y=y_center + y_radius * np.sin(t),\n",
    "                fill=\"toself\",\n",
    "                fillcolor=colors[i],\n",
    "                opacity=0.05,\n",
    "                line=dict(color=colors[i], width=1),\n",
    "                showlegend=False,\n",
    "                hoverinfo=\"skip\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add data points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[win_rate_results[model][\"mean\"] * 100 for model in models],\n",
    "            y=[elo_results[model][\"mean\"] for model in models],\n",
    "            mode=\"markers+text\",\n",
    "            marker=dict(\n",
    "                size=20, color=colors[: len(models)], line=dict(width=1, color=\"black\")\n",
    "            ),\n",
    "            text=model_labels,\n",
    "            textposition=textpositions[: len(models)],\n",
    "            textfont=dict(family=\"Computer Modern\"),\n",
    "            name=\"Models\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        font=dict(family=\"Computer Modern\", size=14),\n",
    "        xaxis=dict(\n",
    "            title=\"Win Rate (%)\",\n",
    "            gridcolor=\"lightgray\",\n",
    "            showgrid=True,\n",
    "            zeroline=True,\n",
    "            zerolinecolor=\"black\",\n",
    "            showline=True,\n",
    "            linewidth=2,\n",
    "            linecolor=\"black\",\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Deception ELO\",\n",
    "            gridcolor=\"lightgray\",\n",
    "            showgrid=True,\n",
    "            zeroline=True,\n",
    "            zerolinecolor=\"black\",\n",
    "            showline=True,\n",
    "            linewidth=2,\n",
    "            linecolor=\"black\",\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        width=800,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    # Set axis ranges\n",
    "    min_win = min([win_rate_results[model][\"ci_lower\"] * 100 for model in models])\n",
    "    max_win = max([win_rate_results[model][\"ci_upper\"] * 100 for model in models])\n",
    "    min_elo = min([elo_results[model][\"ci_lower\"] for model in models])\n",
    "    max_elo = max([elo_results[model][\"ci_upper\"] for model in models])\n",
    "    x_padding = (max_win - min_win) * 0.03\n",
    "    y_padding = (max_elo - min_elo) * 0\n",
    "\n",
    "    fig.update_xaxes(range=[min_win, max_win + x_padding])\n",
    "    fig.update_yaxes(range=[min_elo - y_padding, max_elo + y_padding])\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_elo_vs_winrate_with_ci(elo_results, win_rate_results, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save figure as pdf\n",
    "fig.write_image(\"plots/elo_vs_winrate.pdf\", format=\"pdf\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Claude 3.7, the first hybrid-thinking model, is the most deceptive yet.\n",
    "- Deepseek R1, a reinforcement learning CoT thinking model is the best at winning, but slightly worse than Claude 3.7 at deception.\n",
    "- Smaller models win lesser (and are less deception-capable in general).\n",
    "- Distilling small models using DeepSeek makes them much more powerful at deception capability.\n",
    "- Gemini and o3-mini-high are able to gte good win rates without being as deceptive (which means they win more as a crewmate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaleido\n",
    "import plotly\n",
    "\n",
    "print(kaleido.__version__)\n",
    "print(plotly.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_elo_vs_winrate_with_scatter_ci(\n",
    "    elo_results: Dict, win_rate_results: Dict, models: List[str]\n",
    ") -> go.Figure:\n",
    "    colors = [\n",
    "        \"#1f77b4\",\n",
    "        \"#d62728\",\n",
    "        \"#2ca02c\",\n",
    "        \"#9467bd\",\n",
    "        \"#8c564b\",\n",
    "        \"#e377c2\",\n",
    "        \"#7f7f7f\",\n",
    "        \"#bcbd22\",\n",
    "        \"#17becf\",\n",
    "        \"#ff7f0e\",\n",
    "        \"#ff9896\",\n",
    "        \"#aec7e8\",\n",
    "    ]\n",
    "    model_labels = [model.split(\"/\")[-1] for model in models]\n",
    "    textpositions = [\n",
    "        \"top left\",\n",
    "        \"top center\",\n",
    "        \"middle right\",\n",
    "        \"middle right\",\n",
    "        \"middle right\",\n",
    "        \"bottom right\",\n",
    "        \"bottom right\",\n",
    "        \"bottom center\",\n",
    "        \"middle left\",\n",
    "        \"bottom right\",\n",
    "        \"top center\",\n",
    "    ]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add scatter points for bootstrap samples\n",
    "    for i, model in enumerate(models):\n",
    "        # Add scatter points for bootstrap samples with low opacity\n",
    "        if \"samples\" in elo_results[model] and \"samples\" in win_rate_results[model]:\n",
    "            elo_samples = elo_results[model][\"samples\"]\n",
    "            winrate_samples = (\n",
    "                win_rate_results[model][\"samples\"] * 100\n",
    "            )  # Convert to percentage\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[wr * 100 for wr in winrate_samples],\n",
    "                    y=elo_samples,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(color=colors[i], size=2, opacity=0.4),\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Add data points for model means\n",
    "    for i, model in enumerate(models):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[win_rate_results[model][\"mean\"] * 100],\n",
    "                y=[elo_results[model][\"mean\"]],\n",
    "                mode=\"markers+text\",\n",
    "                marker=dict(\n",
    "                    size=15, color=colors[i], line=dict(width=1.5, color=\"black\")\n",
    "                ),\n",
    "                text=model_labels[i],\n",
    "                textposition=textpositions[i],\n",
    "                textfont=dict(\n",
    "                    family=\"Computer Modern\",\n",
    "                    size=15,\n",
    "                    color=f\"rgba({int(int(colors[i][1:3], 16) / 2)}, {int(int(colors[i][3:5], 16) / 2)}, {int(int(colors[i][5:7], 16) / 2)}, 1)\",\n",
    "                    weight=500,\n",
    "                    # variant='small-caps'\n",
    "                    shadow=\"2px 2px 2px rgba(255, 255, 255, 0.8)\",\n",
    "                ),\n",
    "                name=\"Models\",\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout with LaTeX-style fonts\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        font=dict(family=\"Serif\", size=14),\n",
    "        xaxis=dict(\n",
    "            title=\"Win Rate (%)\",\n",
    "            titlefont=dict(family=\"Serif\", size=16),\n",
    "            gridcolor=\"lightgray\",\n",
    "            showgrid=True,\n",
    "            zeroline=True,\n",
    "            zerolinecolor=\"black\",\n",
    "            showline=True,\n",
    "            linewidth=2,\n",
    "            linecolor=\"black\",\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Deception ELO\",\n",
    "            titlefont=dict(family=\"sans-serif\", size=16),\n",
    "            gridcolor=\"lightgray\",\n",
    "            showgrid=True,\n",
    "            zeroline=True,\n",
    "            zerolinecolor=\"black\",\n",
    "            showline=True,\n",
    "            linewidth=2,\n",
    "            linecolor=\"black\",\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        width=800,\n",
    "        height=600,\n",
    "        plot_bgcolor=\"#fafaf7\",\n",
    "        # paper_bgcolor='#ebdbbc'\n",
    "    )\n",
    "    # fig.update_xaxes(range=[36, 64])\n",
    "    # fig.update_yaxes(range=[1650, 1930])\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_elo_vs_winrate_with_scatter_ci(elo_results, win_rate_results, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save figure as pdf\n",
    "pio.write_image(\n",
    "    fig,\n",
    "    \"plots/elo_vs_winrate_scatter.pdf\",\n",
    "    format=\"pdf\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    engine=\"kaleido\",\n",
    ")\n",
    "pio.write_image(\n",
    "    fig,\n",
    "    \"plots/elo_vs_winrate_scatter.png\",\n",
    "    format=\"png\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    engine=\"kaleido\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expt 2: Deception ELO v Detection ELO\n",
    "\n",
    "As an indication of how the frontier is pushing for more deception capability than detection capability. Will do confidence intervals on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_elo(winner_elo, loser_elo):\n",
    "    expected_win = 1 / (1 + 10 ** ((loser_elo - winner_elo) / 400))\n",
    "    change = K * (1 - expected_win)\n",
    "    return winner_elo + change, loser_elo - change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_ELO = 1500\n",
    "K = 32\n",
    "impostor_elo = collections.defaultdict(lambda: BASE_ELO)\n",
    "crewmate_elo = collections.defaultdict(lambda: BASE_ELO)\n",
    "win_counts = collections.defaultdict(lambda: {\"wins\": 0, \"games\": 0})\n",
    "\n",
    "for idx, game in enumerate(games):\n",
    "    if idx % (len(games) // 10) == 0:\n",
    "        print(f\"Processing game {idx}/{len(games)}.\")\n",
    "    impostor_models = []\n",
    "    crewmate_models = []\n",
    "    all_models = []\n",
    "    impostor_won = game[\"winner\"] == 1 or game[\"winner\"] == 4\n",
    "\n",
    "    for player in game:\n",
    "        if player.startswith(\"Player\"):\n",
    "            model = game[player][\"model\"]\n",
    "            all_models.append(model)\n",
    "            if game[player][\"identity\"] == \"Impostor\":\n",
    "                impostor_models.append(model)\n",
    "            else:\n",
    "                crewmate_models.append(model)\n",
    "\n",
    "    # Update Elo for both roles\n",
    "    if impostor_models and crewmate_models:\n",
    "        avg_crewmate_elo = sum(crewmate_elo[m] for m in crewmate_models) / len(\n",
    "            crewmate_models\n",
    "        )\n",
    "        avg_impostor_elo = sum(impostor_elo[m] for m in impostor_models) / len(\n",
    "            impostor_models\n",
    "        )\n",
    "\n",
    "        # Cache current Elo values\n",
    "        impostor_elo_updates = {}\n",
    "        crewmate_elo_updates = {}\n",
    "\n",
    "        # Calculate updates for impostors\n",
    "        for impostor in impostor_models:\n",
    "            if impostor_won:\n",
    "                new_impostor, _ = update_elo(impostor_elo[impostor], avg_crewmate_elo)\n",
    "            else:\n",
    "                _, new_impostor = update_elo(avg_crewmate_elo, impostor_elo[impostor])\n",
    "            impostor_elo_updates[impostor] = new_impostor\n",
    "\n",
    "        # Calculate updates for crewmates\n",
    "        for crewmate in crewmate_models:\n",
    "            if not impostor_won:\n",
    "                new_crewmate, _ = update_elo(crewmate_elo[crewmate], avg_impostor_elo)\n",
    "            else:\n",
    "                _, new_crewmate = update_elo(avg_impostor_elo, crewmate_elo[crewmate])\n",
    "            crewmate_elo_updates[crewmate] = new_crewmate\n",
    "\n",
    "        # Apply all updates at once\n",
    "        for impostor, new_elo in impostor_elo_updates.items():\n",
    "            impostor_elo[impostor] = new_elo\n",
    "        for crewmate, new_elo in crewmate_elo_updates.items():\n",
    "            crewmate_elo[crewmate] = new_elo\n",
    "\n",
    "    # Update win counts for all players\n",
    "    for model in all_models:\n",
    "        win_counts[model][\"games\"] += 1\n",
    "        if (model in impostor_models and impostor_won) or (\n",
    "            model not in impostor_models and not impostor_won\n",
    "        ):\n",
    "            win_counts[model][\"wins\"] += 1\n",
    "\n",
    "\n",
    "def get_win_rates():\n",
    "    return {\n",
    "        model: win_counts[model][\"wins\"] / win_counts[model][\"games\"]\n",
    "        for model in win_counts\n",
    "        if win_counts[model][\"games\"] > 0\n",
    "    }\n",
    "\n",
    "\n",
    "impostor_elo = [impostor_elo[m] for m in models]\n",
    "crewmate_elo = [crewmate_elo[m] for m in models]\n",
    "win_rates = get_win_rates()\n",
    "win_rates = [win_rates[m] for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_elo_vs_elo(impostor_elo, crewmate_elo):\n",
    "    # models = ['anthropic/claude-3.5-sonnet', 'anthropic/claude-3.7-sonnet', 'deepseek/deepseek-r1', 'deepseek/deepseek-r1-distill-llama-70b', 'google/gemini-2.0-flash-001', 'meta-llama/llama-3.3-70b-instruct', 'microsoft/phi-4', 'mistralai/mistral-7b-instruct', 'openai/gpt-4o-mini', 'openai/o3-mini-high', 'qwen/qwen-2.5-7b-instruct']\n",
    "    colors = [\n",
    "        \"#1f77b4\",\n",
    "        \"#d62728\",\n",
    "        \"#2ca02c\",\n",
    "        \"#9467bd\",\n",
    "        \"#8c564b\",\n",
    "        \"#e377c2\",\n",
    "        \"#7f7f7f\",\n",
    "        \"#bcbd22\",\n",
    "        \"#17becf\",\n",
    "        \"#ff7f0e\",\n",
    "        \"#7f7f7f\",\n",
    "        \"#bcbd22\",\n",
    "        \"#17becf\",\n",
    "        \"#ff7f0e\",\n",
    "    ]\n",
    "    textpositions = [\n",
    "        \"top center\",\n",
    "        \"top center\",\n",
    "        \"middle left\",\n",
    "        \"top center\",\n",
    "        \"top center\",\n",
    "        \"bottom center\",\n",
    "        \"top center\",\n",
    "        \"top center\",\n",
    "        \"middle right\",\n",
    "        \"middle right\",\n",
    "        \"bottom left\",\n",
    "    ]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=crewmate_elo,\n",
    "            y=impostor_elo,\n",
    "            mode=\"markers+text\",\n",
    "            marker=dict(\n",
    "                size=16,\n",
    "                color=colors[: len(impostor_elo)],\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "            ),\n",
    "            text=[model.split(\"/\")[-1] for model in models],\n",
    "            textposition=textpositions[: len(impostor_elo)],\n",
    "            textfont=dict(family=\"Computer Modern\"),\n",
    "            name=\"\",\n",
    "        )\n",
    "    )\n",
    "    # min_val, max_val = min(min(crewmate_elo), min(impostor_elo)), max(max(crewmate_elo), max(impostor_elo))\n",
    "    # fig.add_trace(go.Scatter(x=[min_val, max_val], y=[min_val, max_val], mode='lines', line=dict(color='red', dash='dot'), name='Balance'))\n",
    "    x1, y1 = 1450 + 200, 1527 + 1.272 * 200\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[1450 - 100, x1],\n",
    "            y=[1527 - 1.272 * 100, y1],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"red\", dash=\"dot\"),\n",
    "            name=\"Balance\",\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        font=dict(family=\"Computer Modern\", size=14),\n",
    "        xaxis=dict(\n",
    "            title=r\"Detection ELO\",\n",
    "            gridcolor=\"lightgray\",\n",
    "            showgrid=True,\n",
    "            zeroline=True,\n",
    "            zerolinecolor=\"black\",\n",
    "            showline=True,\n",
    "            linewidth=2,\n",
    "            linecolor=\"black\",\n",
    "            dtick=50,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=r\"Deception ELO\",\n",
    "            gridcolor=\"lightgray\",\n",
    "            showgrid=True,\n",
    "            zeroline=True,\n",
    "            zerolinecolor=\"black\",\n",
    "            showline=True,\n",
    "            linewidth=2,\n",
    "            linecolor=\"black\",\n",
    "            dtick=50,\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        width=600,\n",
    "        height=600,\n",
    "    )\n",
    "    # fig.update_xaxes(range=[1330, 1600])\n",
    "    # fig.update_yaxes(range=[1350, 1680])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_elo_vs_elo(impostor_elo, crewmate_elo)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times a model wins as impostor or crewmate\n",
    "impostor_wins = 0\n",
    "crewmate_wins = 0\n",
    "\n",
    "# calculate total wins\n",
    "\n",
    "for game in games:\n",
    "    if game[\"winner\"] == 1 or game[\"winner\"] == 4:\n",
    "        impostor_wins += 1\n",
    "    else:\n",
    "        crewmate_wins += 1\n",
    "print(f\"Impostor wins: {impostor_wins}\")\n",
    "print(f\"Crewmate wins: {crewmate_wins}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = impostor_wins / crewmate_wins\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of impostor elo and crewmate elo\n",
    "print(np.mean(impostor_elo))\n",
    "print(np.mean(crewmate_elo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo v Elo with Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "def bootstrap_analysis_elo_elo(\n",
    "    games: List[Dict],\n",
    "    models: List[str],\n",
    "    n_bootstrap: int = 1000,\n",
    "    confidence_level: float = 0.95,\n",
    "    K: int = 32,\n",
    "    BASE_ELO: int = 1500,\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Perform bootstrap analysis on game data to get confidence intervals for Impostor and Crewmate ELO ratings.\n",
    "\n",
    "    Args:\n",
    "        games: List of game dictionaries containing game results\n",
    "        models: List of model names to analyze\n",
    "        n_bootstrap: Number of bootstrap samples to generate\n",
    "        confidence_level: Confidence level for intervals (e.g., 0.95 for 95% CI)\n",
    "        K: ELO K-factor\n",
    "        BASE_ELO: Base ELO rating\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (impostor_elo_results, crewmate_elo_results) dictionaries containing mean and CI for each model\n",
    "    \"\"\"\n",
    "    # Initialize results dictionaries\n",
    "    impostor_elo_results = {model: {\"samples\": []} for model in models}\n",
    "    crewmate_elo_results = {model: {\"samples\": []} for model in models}\n",
    "\n",
    "    def update_elo(winner_elo, loser_elo):\n",
    "        expected_win = 1 / (1 + 10 ** ((loser_elo - winner_elo) / 400))\n",
    "        change = K * (1 - expected_win)\n",
    "        return winner_elo + change, loser_elo - change\n",
    "\n",
    "    for bootstrap_iter in range(n_bootstrap):\n",
    "        if bootstrap_iter % 100 == 0:\n",
    "            print(f\"Bootstrap iteration {bootstrap_iter}/{n_bootstrap}\")\n",
    "\n",
    "        bootstrap_games = random.choices(games, k=len(games))\n",
    "        impostor_elo = {model: BASE_ELO for model in models}\n",
    "        crewmate_elo = {model: BASE_ELO for model in models}\n",
    "\n",
    "        for game in bootstrap_games:\n",
    "            impostor_models = []\n",
    "            crewmate_models = []\n",
    "\n",
    "            impostor_won = game[\"winner\"] == 1 or game[\"winner\"] == 4\n",
    "\n",
    "            for player in game:\n",
    "                if player.startswith(\"Player\"):\n",
    "                    model = game[player][\"model\"]\n",
    "                    if game[player][\"identity\"] == \"Impostor\":\n",
    "                        impostor_models.append(model)\n",
    "                    else:\n",
    "                        crewmate_models.append(model)\n",
    "\n",
    "            if impostor_models and crewmate_models:\n",
    "                # Process each impostor-crewmate pair\n",
    "                for impostor in impostor_models:\n",
    "                    for crewmate in crewmate_models:\n",
    "                        if impostor_won:\n",
    "                            # Impostor won against this crewmate\n",
    "                            impostor_elo[impostor], crewmate_elo[crewmate] = update_elo(\n",
    "                                impostor_elo[impostor], crewmate_elo[crewmate]\n",
    "                            )\n",
    "                        else:\n",
    "                            # Crewmate won against this impostor\n",
    "                            crewmate_elo[crewmate], impostor_elo[impostor] = update_elo(\n",
    "                                crewmate_elo[crewmate], impostor_elo[impostor]\n",
    "                            )\n",
    "\n",
    "        # Store final ELO ratings for this bootstrap sample\n",
    "        for model in models:\n",
    "            impostor_elo_results[model][\"samples\"].append(\n",
    "                impostor_elo.get(model, BASE_ELO)\n",
    "            )\n",
    "            crewmate_elo_results[model][\"samples\"].append(\n",
    "                crewmate_elo.get(model, BASE_ELO)\n",
    "            )\n",
    "\n",
    "    # Calculate statistics\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = alpha / 2 * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "\n",
    "    for model in models:\n",
    "        # Impostor ELO statistics\n",
    "        impostor_samples = impostor_elo_results[model][\"samples\"]\n",
    "        impostor_elo_results[model][\"mean\"] = np.mean(impostor_samples)\n",
    "        impostor_elo_results[model][\"ci_lower\"] = np.percentile(\n",
    "            impostor_samples, lower_percentile\n",
    "        )\n",
    "        impostor_elo_results[model][\"ci_upper\"] = np.percentile(\n",
    "            impostor_samples, upper_percentile\n",
    "        )\n",
    "\n",
    "        # Crewmate ELO statistics\n",
    "        crewmate_samples = crewmate_elo_results[model][\"samples\"]\n",
    "        crewmate_elo_results[model][\"mean\"] = np.mean(crewmate_samples)\n",
    "        crewmate_elo_results[model][\"ci_lower\"] = np.percentile(\n",
    "            crewmate_samples, lower_percentile\n",
    "        )\n",
    "        crewmate_elo_results[model][\"ci_upper\"] = np.percentile(\n",
    "            crewmate_samples, upper_percentile\n",
    "        )\n",
    "\n",
    "    return impostor_elo_results, crewmate_elo_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_elo, crew_elo = bootstrap_analysis_elo_elo(games, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def plot_elo_vs_elo_with_ci(\n",
    "    impostor_elo_results: Dict, crewmate_elo_results: Dict, models: List[str]\n",
    ") -> go.Figure:\n",
    "    colors = [\n",
    "        \"#1f77b4\",\n",
    "        \"#d62728\",\n",
    "        \"#2ca02c\",\n",
    "        \"#9467bd\",\n",
    "        \"#8c564b\",\n",
    "        \"#e377c2\",\n",
    "        \"#7f7f7f\",\n",
    "        \"#bcbd22\",\n",
    "        \"#17becf\",\n",
    "        \"#ff7f0e\",\n",
    "        \"#ff9896\",\n",
    "        \"#aec7e8\",\n",
    "    ]\n",
    "    model_labels = [model.split(\"/\")[-1] for model in models]\n",
    "    textpositions = [\n",
    "        \"top right\",\n",
    "        \"top left\",\n",
    "        \"middle right\",\n",
    "        \"top center\",\n",
    "        \"middle left\",\n",
    "        \"bottom center\",\n",
    "        \"top center\",\n",
    "        \"top center\",\n",
    "        \"middle right\",\n",
    "        \"middle right\",\n",
    "        \"bottom left\",\n",
    "    ]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add scatter points for bootstrap samples\n",
    "    for i, model in enumerate(models):\n",
    "        if (\n",
    "            \"samples\" in crewmate_elo_results[model]\n",
    "            and \"samples\" in impostor_elo_results[model]\n",
    "        ):\n",
    "            crewmate_samples = crewmate_elo_results[model][\"samples\"]\n",
    "            impostor_samples = impostor_elo_results[model][\"samples\"]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=crewmate_samples,\n",
    "                    y=impostor_samples,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(color=colors[i], size=3, opacity=0.3),\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Add mean data points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[crewmate_elo_results[model][\"mean\"] for model in models],\n",
    "            y=[impostor_elo_results[model][\"mean\"] for model in models],\n",
    "            mode=\"markers+text\",\n",
    "            marker=dict(\n",
    "                size=16, color=colors[: len(models)], line=dict(width=1, color=\"black\")\n",
    "            ),\n",
    "            text=model_labels,\n",
    "            textposition=textpositions[: len(models)],\n",
    "            textfont=dict(family=\"Computer Modern\", size=17),\n",
    "            name=\"Models\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add balance line\n",
    "    x1, y1 = 1474 + 200, 1526 + 1.272 * 200\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[1474 - 100, x1],\n",
    "            y=[1526 - 1.272 * 100, y1],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"red\", dash=\"dot\"),\n",
    "            name=\"Balance\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        font=dict(family=\"Computer Modern\", size=14),\n",
    "        xaxis=dict(\n",
    "            title=r\"Detection ELO\",\n",
    "            gridcolor=\"lightgray\",\n",
    "            showgrid=True,\n",
    "            zeroline=True,\n",
    "            zerolinecolor=\"black\",\n",
    "            showline=True,\n",
    "            linewidth=2,\n",
    "            linecolor=\"black\",\n",
    "            dtick=50,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=r\"Deception ELO\",\n",
    "            gridcolor=\"lightgray\",\n",
    "            showgrid=True,\n",
    "            zeroline=True,\n",
    "            zerolinecolor=\"black\",\n",
    "            showline=True,\n",
    "            linewidth=2,\n",
    "            linecolor=\"black\",\n",
    "            dtick=50,\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        width=600,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    # Set axis ranges\n",
    "    # fig.update_xaxes(range=[1300, 1600])\n",
    "    # fig.update_yaxes(range=[1400, 1650])\n",
    "    fig.update_xaxes(range=[900, 2000])\n",
    "    fig.update_yaxes(range=[900, 2000])\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_elo_vs_elo_with_ci(imp_elo, crew_elo, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save figure as pdf\n",
    "fig.write_image(\"plots/elo_vs_elo.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of impostor elo and crewmate elo\n",
    "print(np.mean(impostor_elo))\n",
    "print(np.mean(crewmate_elo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of all imp_elo\n",
    "print(np.mean([imp_elo[model][\"mean\"] for model in models]))\n",
    "# mean of all crew_elo\n",
    "print(np.mean([crew_elo[model][\"mean\"] for model in models]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amongus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}